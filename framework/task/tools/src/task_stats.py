#!/usr/bin/env python3  # noqa: EXE001
# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "orjson>=3.8.0",
#     "numpy>=2.3.2",
#     "matplotlib>=3.5.0",
# ]
# ///

"""
Task and Trigger Statistics Analyzer.

Reads JSON log files generated by TaskMonitor's or TimedTrigger's write_stats_to_file
function and creates CDFs of jitters/latencies and execution times plus basic summary
statistics.

Supports:
- Task execution records (from TaskMonitor)
- Trigger execution records (from TimedTrigger)
- Mixed files with both task and trigger records

The analyzer automatically detects record types by examining the keys in each record.
For mixed files, it analyzes all records of both types and generates separate analysis
and plots for each type.

Usage:
    # Standalone with uv (no installation required):
    uv run task_stats.py <json_file> [options]

    # Direct Python execution:
    python task_stats.py <json_file> [options]

    # After pip install:
    task_stats <json_file> [options]
"""

import argparse
import logging
import os
import sys
from pathlib import Path
from typing import Any

# Force non-interactive backend before importing matplotlib
# Prevents conflicts when called from Jupyter notebooks
os.environ["MPLBACKEND"] = "Agg"

import matplotlib.pyplot as plt
import numpy as np
import orjson


class WarningErrorHandler(logging.Handler):
    """Custom logging handler to track WARNING and ERROR messages."""

    def __init__(self) -> None:
        super().__init__()
        self.has_warnings = False
        self.has_errors = False

    def emit(self, record: logging.LogRecord) -> None:
        """Track WARNING and ERROR level messages."""
        if record.levelno >= logging.ERROR:
            self.has_errors = True
        elif record.levelno >= logging.WARNING:
            self.has_warnings = True

    def has_warning_or_error(self) -> bool:
        """Return True if any WARNING or ERROR messages were logged."""
        return self.has_warnings or self.has_errors


class TaskStatsAnalyzer:
    """Analyzer for task and trigger execution statistics from JSON files.

    Automatically detects and processes both record types:
    - Records with 'task_name' key are treated as task execution records
    - Records with 'trigger_count' key are treated as trigger execution records
    - Mixed files are fully supported - analyzer processes all records of both types
    - Separate analysis is provided for each record type found
    """

    # Constants for minimum sample sizes
    MIN_SAMPLES_FOR_PLOTS = 10
    MIN_TRIGGERS_FOR_SEGMENTS = 100

    def __init__(self, json_file: Path) -> None:
        self.json_file = json_file
        self.version = None
        self.format_type = None
        self.task_records = []
        self.trigger_records = []
        self.plot_count = 0
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

    def load_data(self) -> bool:  # noqa: PLR0912, PLR0915
        """Load and parse the JSON statistics file."""
        try:
            with self.json_file.open("rb") as f:
                lines = f.readlines()

            if not lines:
                self.logger.error(f"Empty file {self.json_file}")
                return False

            # Parse version header (first line) - may contain version info
            start_line = 0
            try:
                header = orjson.loads(lines[0])
                if "version" in header:
                    self.version = header.get("version", "unknown")
                    start_line = 1  # Skip header line
                    self.logger.info(f"File version: {self.version}")
                else:
                    # First line is an execution record, not a header
                    self.version = "unknown"
            except (orjson.JSONDecodeError, KeyError) as e:
                self.logger.debug(f"First line is not a header: {e}")
                self.version = "unknown"

            # Parse execution records (all lines except header if present)
            for line_num, line in enumerate(lines[start_line:], start=start_line + 1):
                try:
                    record = orjson.loads(line.strip())
                    # Skip metadata lines (warnings, etc.)
                    if any(key in record for key in ["warning", "version"]):
                        continue

                    # Classify and store records by type
                    if "trigger_count" in record:
                        self.trigger_records.append(record)
                    elif "task_name" in record:
                        self.task_records.append(record)
                    else:
                        self.logger.warning(f"Unknown record type: {list(record.keys())}")

                except orjson.JSONDecodeError as e:
                    self.logger.warning(f"Could not parse line {line_num}: {e}")

            # Determine file type and format
            task_count = len(self.task_records)
            trigger_count = len(self.trigger_records)

            if task_count > 0 and trigger_count > 0:
                self.format_type = "mixed_execution_records"
                self.logger.info(
                    f"File contains: {task_count} task records, {trigger_count} trigger records"
                )
            elif task_count > 0:
                self.format_type = "task_execution_records"
                self.logger.info(f"File contains: {task_count} task records")
            elif trigger_count > 0:
                self.format_type = "trigger_execution_records"
                self.logger.info(f"File contains: {trigger_count} trigger records")
            else:
                self.logger.warning("No execution records found")
                self.format_type = "unknown"
                return False

            total_records = task_count + trigger_count
            self.logger.info(
                f"Loaded {total_records} total execution records "
                f"({task_count} tasks, {trigger_count} triggers)"
            )
        except FileNotFoundError:
            self.logger.exception(f"File {self.json_file} not found")
            return False
        except Exception:
            self.logger.exception("Error reading file")
            return False
        else:
            return total_records > 0

    def get_task_execution_times_us(self) -> list[float]:
        """Get task execution times in microseconds."""
        return [record["duration_ns"] / 1000.0 for record in self.task_records]

    def get_task_jitters_us(self) -> list[float]:
        """Get task jitter values in microseconds (absolute value)."""
        return [abs(record["jitter_ns"]) / 1000.0 for record in self.task_records]

    def get_trigger_execution_times_us(self) -> list[float]:
        """Get trigger callback duration times in microseconds."""
        return [record["callback_duration_ns"] / 1000.0 for record in self.trigger_records]

    def get_trigger_jitters_us(self) -> list[float]:
        """Get trigger latency values in microseconds (absolute value)."""
        return [abs(record["latency_ns"]) / 1000.0 for record in self.trigger_records]

    def calculate_statistics(self, values: list[float]) -> dict[str, float]:
        """Calculate comprehensive statistics for a list of values.

            Args:
            values: List of numerical values to analyze

        Returns
        -------
            Dictionary containing statistical measures:
                - count: Number of values
                - min: Minimum value
                - p10: 10th percentile
                - median: 50th percentile (median)
                - mean: Arithmetic mean
                - p95: 95th percentile
                - p99: 99th percentile
                - max: Maximum value
                - std: Standard deviation (0.0 for single values)

            Returns empty dict if no values provided.
        """
        if not values:
            return {}

        values_array = np.array(values)
        return {
            "count": len(values),
            "min": float(np.min(values_array)),
            "p10": float(np.percentile(values_array, 10)),
            "median": float(np.percentile(values_array, 50)),
            "mean": float(np.mean(values_array)),
            "p95": float(np.percentile(values_array, 95)),
            "p99": float(np.percentile(values_array, 99)),
            "max": float(np.max(values_array)),
            "std": float(np.std(values_array)) if len(values) > 1 else 0.0,
        }

    def _log_statistics(self, stats: dict[str, float], title: str) -> None:
        """Log statistics in a consistent format.

        Args:
            stats: Statistics dictionary from calculate_statistics
            title: Title to display before statistics
        """
        self.logger.info(f"{title}:")
        self.logger.info(f"  Average: {stats['mean']:.3f} us")
        self.logger.info(f"  Median:  {stats['median']:.3f} us")
        self.logger.info(f"  10th:    {stats['p10']:.3f} us")
        self.logger.info(f"  95th:    {stats['p95']:.3f} us")
        self.logger.info(f"  99th:    {stats['p99']:.3f} us")
        self.logger.info(f"  Min:     {stats['min']:.3f} us")
        self.logger.info(f"  Max:     {stats['max']:.3f} us")
        self.logger.info(f"  Std:     {stats['std']:.3f} us")

    def _sanitize_name_for_filename(self, name: str) -> str:
        """Sanitize a name for use in filenames.

        Args:
            name: The name to sanitize

        Returns
        -------
            Sanitized name suitable for filenames
        """
        return name.replace("/", "_").replace(" ", "_").replace(":", "_")

    def _create_plot_statistics_and_markers(self, values_sorted: np.ndarray) -> list[tuple]:
        """Create statistics and markers for plotting.

        Args:
            values_sorted: Sorted array of values

        Returns
        -------
            List of (value, label, color) tuples for plotting markers
        """
        # Calculate basic statistics
        min_val = float(np.min(values_sorted))
        max_val = float(np.max(values_sorted))
        median_val = float(np.median(values_sorted))
        mean_val = float(np.mean(values_sorted))

        statistics = [
            (min_val, "Min", "blue"),
            (median_val, "Median", "green"),
            (mean_val, "Mean", "red"),
            (max_val, "Max", "purple"),
        ]

        # Mark percentiles
        percentiles = [95, 99]
        percentile_colors = ["orange", "brown"]

        for p, color in zip(percentiles, percentile_colors, strict=False):
            value = float(np.percentile(values_sorted, p))
            statistics.append((value, f"{p}th %ile", color))

        return statistics

    def _group_tasks_by_name(self, tasks: list[dict[str, Any]]) -> dict[str, list[dict[str, Any]]]:
        """Group tasks by task name.

        Args:
            tasks: List of task records

        Returns
        -------
            Dictionary mapping task names to lists of task records
        """
        task_breakdown = {}
        for task in tasks:
            name = task["task_name"]
            if name not in task_breakdown:
                task_breakdown[name] = []
            task_breakdown[name].append(task)
        return task_breakdown

    def print_summary(self) -> None:
        """Print comprehensive summary statistics for all record types found."""
        if not self.task_records and not self.trigger_records:
            self.logger.warning("No execution records to analyze")
            return

        if self.task_records:
            self._print_task_summary()

        if self.trigger_records:
            if self.task_records:  # Add separator if we printed task summary first
                self.logger.info(f"\n{'=' * 50}\n")
            self._print_trigger_summary()

    def _print_task_summary(self) -> None:
        """Print task execution statistics summary."""
        self.logger.info("====== TASK EXECUTION RECORDS ======")
        self.logger.info("====== Task Execution Statistics ======")
        self.logger.info(f"Total tasks executed: {len(self.task_records)}")

        # Separate tasks into root and dependent categories
        root_tasks = [r for r in self.task_records if r["dependency_generation"] == 0]
        dependent_tasks = [r for r in self.task_records if r["dependency_generation"] > 0]

        # Print stats for each category
        self._print_category_stats(root_tasks, "Root")
        self._print_category_stats(dependent_tasks, "Dependent")

        # Print graph-level statistics
        self._print_graph_statistics()

        self.logger.info("======================================")

        # Overall worker distribution
        worker_distribution = {}
        for record in self.task_records:
            worker_id = record["worker"]
            if worker_id not in worker_distribution:
                worker_distribution[worker_id] = 0
            worker_distribution[worker_id] += 1

        self.logger.info("TaskMonitor data structure size: active_tasks=0")

    def _print_category_stats(self, tasks: list[dict[str, Any]], category_name: str) -> None:
        """Print statistics for a category of tasks (Root or Dependent)."""
        if not tasks:
            self.logger.debug(f"No {category_name} tasks were executed")
            return

        self.logger.info(f"===== {category_name} Tasks ({len(tasks)} tasks) =====")

        # Jitter statistics
        jitters = [abs(task["jitter_ns"]) / 1000.0 for task in tasks]
        jitter_stats = self.calculate_statistics(jitters)

        self._log_statistics(jitter_stats, "Overall Jitter Statistics")

        # Jitter by worker
        self.logger.info("Jitter by Worker:")
        worker_jitters = {}
        for task in tasks:
            worker_id = task["worker"]
            jitter_us = abs(task["jitter_ns"]) / 1000.0
            if worker_id not in worker_jitters:
                worker_jitters[worker_id] = []
            worker_jitters[worker_id].append(jitter_us)

        for worker_id in sorted(worker_jitters.keys()):
            jitters_list = worker_jitters[worker_id]
            avg_jitter = sum(jitters_list) / len(jitters_list)
            max_jitter = max(jitters_list)
            self.logger.info(
                f"  Worker {worker_id}: {avg_jitter:.3f} us avg, "
                f"{max_jitter:.3f} us max, {len(jitters_list)} tasks"
            )

        # Execution time statistics
        execution_times = [task["duration_ns"] / 1000.0 for task in tasks]
        exec_stats = self.calculate_statistics(execution_times)

        self._log_statistics(exec_stats, "Overall Execution Time Distribution")

        # Execution time distribution by task
        self.logger.info("Execution Time Distribution by Task:")
        task_breakdown = self._group_tasks_by_name(tasks)

        for task_name in sorted(task_breakdown.keys()):
            task_records = task_breakdown[task_name]
            task_exec_times = [r["duration_ns"] / 1000.0 for r in task_records]
            task_exec_stats = self.calculate_statistics(task_exec_times)

            # Calculate status counts
            cancelled_count = sum(1 for r in task_records if r["was_cancelled"] == "true")
            failed_count = sum(1 for r in task_records if r["status"] == "Failed")

            self.logger.info(f"  Task '{task_name}' ({len(task_records)} executions):")
            self.logger.info(
                f"    Min: {task_exec_stats['min']:.3f} us, "
                f"Max: {task_exec_stats['max']:.3f} us, "
                f"Avg: {task_exec_stats['mean']:.3f} us"
            )
            self.logger.info(
                f"    Median: {task_exec_stats['median']:.3f} us, "
                f"10th: {task_exec_stats['p10']:.3f} us, "
                f"95th: {task_exec_stats['p95']:.3f} us, "
                f"99th: {task_exec_stats['p99']:.3f} us"
            )
            self.logger.info(f"    Std: {task_exec_stats['std']:.3f} us")
            self.logger.info(
                f"    Cancelled: {cancelled_count} "
                f"({cancelled_count / len(task_records) * 100:.1f}%) "
                f"[includes timeouts], "
                f"Failed: {failed_count} "
                f"({failed_count / len(task_records) * 100:.1f}%)"
            )

        # Task status summary
        cancelled_count = sum(1 for t in tasks if t["was_cancelled"] == "true")
        failed_count = sum(1 for t in tasks if t["status"] == "Failed")

        self.logger.info("Task Status Summary:")
        self.logger.info(
            f"  Cancelled: {cancelled_count} "
            f"({cancelled_count / len(tasks) * 100:.1f}%) [includes timeouts]"
        )
        self.logger.info(f"  Failed: {failed_count} ({failed_count / len(tasks) * 100:.1f}%)")

    def _print_graph_statistics(self) -> None:
        """Print graph-level execution statistics.

        Aggregated across all times scheduled.
        """
        if not self.task_records:
            return

        # Group tasks by graph name (aggregate all times scheduled)
        graph_tasks_map, graph_times_scheduled = self._group_tasks_by_graph()

        self.logger.info("===== Graph Execution Statistics =====")

        for graph_name in sorted(graph_tasks_map.keys()):
            tasks = graph_tasks_map[graph_name]

            if not tasks:
                continue

            # Collect execution times for statistics calculation
            execution_times_us = []
            cancelled_count = 0
            failed_count = 0

            for task in tasks:
                execution_time_us = task["duration_ns"] / 1000.0
                execution_times_us.append(execution_time_us)

                if task["was_cancelled"] == "true":
                    cancelled_count += 1
                if task["status"] == "Failed":
                    failed_count += 1

            # Calculate statistics using the same method as individual tasks
            task_exec_stats = self.calculate_statistics(execution_times_us)
            num_times_scheduled = len(graph_times_scheduled[graph_name])

            self.logger.info(
                f"  Graph '{graph_name}' ({len(tasks)} tasks across "
                f"{num_times_scheduled} times scheduled):"
            )
            self.logger.info(
                f"    Min: {task_exec_stats['min']:.3f} us, "
                f"Max: {task_exec_stats['max']:.3f} us, "
                f"Avg: {task_exec_stats['mean']:.3f} us"
            )
            self.logger.info(
                f"    Median: {task_exec_stats['median']:.3f} us, "
                f"10th: {task_exec_stats['p10']:.3f} us, "
                f"95th: {task_exec_stats['p95']:.3f} us, "
                f"99th: {task_exec_stats['p99']:.3f} us"
            )
            self.logger.info(f"    Std: {task_exec_stats['std']:.3f} us")
            self.logger.info(
                f"    Cancelled: {cancelled_count} "
                f"({cancelled_count / len(tasks) * 100:.1f}%) "
                f"[includes timeouts], "
                f"Failed: {failed_count} "
                f"({failed_count / len(tasks) * 100:.1f}%)"
            )

    def plot_cdf(self, values: list[float], title: str, xlabel: str, output_file: str) -> None:
        """Plot Cumulative Distribution Function."""
        if not values:
            self.logger.warning(f"No values to plot for {title}")
            return

        values_sorted = np.sort(values)
        y_vals = np.arange(1, len(values_sorted) + 1) / len(values_sorted)

        plt.figure(figsize=(12, 7))
        plt.plot(values_sorted, y_vals, linewidth=2, label=f"CDF (n={len(values)})")
        plt.grid(visible=True, alpha=0.3)

        # Create statistics markers
        statistics = self._create_plot_statistics_and_markers(values_sorted)

        # Plot all markers and create legend
        for value, label, color in statistics:
            plt.axvline(
                x=value,
                color=color,
                linestyle="--",
                alpha=0.7,
                label=f"{label}: {value:.3f} µs",
            )

        plt.xlabel(xlabel)
        plt.ylabel("P(X ≤ x)")
        plt.title(title)
        plt.legend(loc="lower right")
        plt.xlim(left=0)
        plt.ylim(0, 1)

        plt.tight_layout()
        plt.savefig(output_file, dpi=150, bbox_inches="tight")
        self.plot_count += 1
        self.logger.info(f"[{self.plot_count}] CDF plot saved to {output_file}")
        plt.close()

    def plot_ccdf(self, values: list[float], title: str, xlabel: str, output_file: str) -> None:
        """Plot Complementary Cumulative Distribution Function (1 - CDF)."""
        if not values:
            self.logger.warning(f"No values to plot for {title}")
            return

        values_sorted = np.sort(values)
        y_vals = 1 - (np.arange(1, len(values_sorted) + 1) / len(values_sorted))

        plt.figure(figsize=(12, 7))
        plt.semilogy(values_sorted, y_vals, linewidth=2, label=f"CCDF (n={len(values)})")
        plt.grid(visible=True, alpha=0.3)

        # Create statistics markers
        statistics = self._create_plot_statistics_and_markers(values_sorted)

        # Plot all markers and create legend
        for value, label, color in statistics:
            plt.axvline(
                x=value,
                color=color,
                linestyle="--",
                alpha=0.7,
                label=f"{label}: {value:.3f} µs",
            )

        plt.xlabel(xlabel)
        plt.ylabel("P(X > x) [log scale]")
        plt.title(title)
        plt.legend(loc="upper right")
        plt.xlim(left=0)

        plt.tight_layout()
        plt.savefig(output_file, dpi=150, bbox_inches="tight")
        self.plot_count += 1
        self.logger.info(f"[{self.plot_count}] CCDF plot saved to {output_file}")
        plt.close()

    def generate_plots(self, output_dir: Path) -> None:
        """Generate comprehensive plots in organized directory structure."""
        # Initialize progress tracking
        self.plot_count = 0
        self.logger.info("Generating plots...")

        # Generate task plots if we have task records
        if self.task_records:
            task_dir = output_dir / "tasks" if self.trigger_records else output_dir
            self.logger.info(f"Processing {len(self.task_records)} task records")
            self._generate_task_plots(task_dir)

        # Generate trigger plots if we have trigger records
        if self.trigger_records:
            trigger_dir = output_dir / "triggers" if self.task_records else output_dir
            self.logger.info(f"Processing {len(self.trigger_records)} trigger records")
            self._generate_trigger_plots(trigger_dir)

        self.logger.info(f"Completed! Generated {self.plot_count} plots.")

    def _generate_task_plots(self, output_dir: Path) -> None:
        """Generate plots for task records."""
        output_dir.mkdir(parents=True, exist_ok=True)

        # Get task data
        execution_times = self.get_task_execution_times_us()
        jitters = self.get_task_jitters_us()

        # Generate overall task plots
        self.plot_cdf(
            execution_times,
            f"Overall Task Execution Times CDF ({len(execution_times)} samples)",
            "Task Execution Times (microseconds)",
            str(output_dir / "overall_execution_times_cdf.png"),
        )

        self.plot_ccdf(
            execution_times,
            f"Overall Task Execution Times CCDF ({len(execution_times)} samples)",
            "Task Execution Times (microseconds)",
            str(output_dir / "overall_execution_times_ccdf.png"),
        )

        self.plot_cdf(
            jitters,
            f"Overall Task Scheduling Jitter CDF ({len(jitters)} samples)",
            "Task Scheduling Jitter (microseconds, absolute)",
            str(output_dir / "overall_jitter_cdf.png"),
        )

        self.plot_ccdf(
            jitters,
            f"Overall Task Scheduling Jitter CCDF ({len(jitters)} samples)",
            "Task Scheduling Jitter (microseconds, absolute)",
            str(output_dir / "overall_jitter_ccdf.png"),
        )

        # Generate task-specific additional plots
        root_tasks = [r for r in self.task_records if r["dependency_generation"] == 0]
        dependent_tasks = [r for r in self.task_records if r["dependency_generation"] > 0]

        if root_tasks:
            self._generate_category_plots(root_tasks, output_dir / "root_tasks", "Root Tasks")

        if dependent_tasks:
            self._generate_category_plots(
                dependent_tasks, output_dir / "dependent_tasks", "Dependent Tasks"
            )

        self._generate_graph_plots(output_dir / "graphs")

    def _generate_category_plots(
        self, tasks: list[dict[str, Any]], category_dir: Path, category_name: str
    ) -> None:
        """Generate plots for a specific category (Root/Dependent).

        Broken down by individual tasks.
        """
        category_dir.mkdir(parents=True, exist_ok=True)

        # Generate overall category plots
        execution_times = [task["duration_ns"] / 1000.0 for task in tasks]
        jitters = [abs(task["jitter_ns"]) / 1000.0 for task in tasks]

        self.plot_cdf(
            execution_times,
            f"{category_name} Execution Times CDF ({len(execution_times)} samples)",
            "Execution Time (microseconds)",
            str(category_dir / "category_execution_times_cdf.png"),
        )

        self.plot_ccdf(
            execution_times,
            f"{category_name} Execution Times CCDF ({len(execution_times)} samples)",
            "Execution Time (microseconds)",
            str(category_dir / "category_execution_times_ccdf.png"),
        )

        self.plot_cdf(
            jitters,
            f"{category_name} Jitter CDF ({len(jitters)} samples)",
            "Jitter (microseconds, absolute)",
            str(category_dir / "category_jitter_cdf.png"),
        )

        self.plot_ccdf(
            jitters,
            f"{category_name} Jitter CCDF ({len(jitters)} samples)",
            "Jitter (microseconds, absolute)",
            str(category_dir / "category_jitter_ccdf.png"),
        )

        # Generate individual task plots
        task_breakdown = self._group_tasks_by_name(tasks)

        for task_name, task_records in task_breakdown.items():
            if (
                len(task_records) < self.MIN_SAMPLES_FOR_PLOTS
            ):  # Skip tasks with too few samples for meaningful plots
                self.logger.debug(
                    f"Skipping plots for {task_name}: only {len(task_records)} samples"
                )
                continue

            task_exec_times = [r["duration_ns"] / 1000.0 for r in task_records]
            task_jitters = [abs(r["jitter_ns"]) / 1000.0 for r in task_records]

            # Clean task name for filename
            safe_task_name = self._sanitize_name_for_filename(task_name)

            # Execution time plots
            self.plot_cdf(
                task_exec_times,
                f"Task '{task_name}' Execution Times CDF ({len(task_exec_times)} samples)",
                "Execution Time (microseconds)",
                str(category_dir / f"{safe_task_name}_execution_times_cdf.png"),
            )

            self.plot_ccdf(
                task_exec_times,
                f"Task '{task_name}' Execution Times CCDF ({len(task_exec_times)} samples)",
                "Execution Time (microseconds)",
                str(category_dir / f"{safe_task_name}_execution_times_ccdf.png"),
            )

            # Jitter plots
            self.plot_cdf(
                task_jitters,
                f"Task '{task_name}' Jitter CDF ({len(task_jitters)} samples)",
                "Jitter (microseconds, absolute)",
                str(category_dir / f"{safe_task_name}_jitter_cdf.png"),
            )

            self.plot_ccdf(
                task_jitters,
                f"Task '{task_name}' Jitter CCDF ({len(task_jitters)} samples)",
                "Jitter (microseconds, absolute)",
                str(category_dir / f"{safe_task_name}_jitter_ccdf.png"),
            )

    def _generate_graph_plots(self, graphs_dir: Path) -> None:
        """Generate CDF and CCDF plots for each graph."""
        graphs_dir.mkdir(parents=True, exist_ok=True)

        if not self.task_records:
            return

        # Group tasks by graph name (aggregate all times scheduled)
        graph_tasks_map, graph_times_scheduled = self._group_tasks_by_graph()

        # Generate plots for each graph
        for graph_name in sorted(graph_tasks_map.keys()):
            tasks = graph_tasks_map[graph_name]

            if (
                len(tasks) < self.MIN_SAMPLES_FOR_PLOTS
            ):  # Skip graphs with too few samples for meaningful plots
                self.logger.debug(
                    f"Skipping plots for graph '{graph_name}': only {len(tasks)} samples"
                )
                continue

            # Collect execution times for this graph
            graph_execution_times = [task["duration_ns"] / 1000.0 for task in tasks]
            num_times_scheduled = len(graph_times_scheduled[graph_name])

            # Clean graph name for filename
            safe_graph_name = self._sanitize_name_for_filename(graph_name)

            # Generate CDF plot
            self.plot_cdf(
                graph_execution_times,
                f"Graph '{graph_name}' Execution Times CDF "
                f"({len(graph_execution_times)} tasks, {num_times_scheduled} sched)",
                "Execution Time (microseconds)",
                str(graphs_dir / f"{safe_graph_name}_execution_times_cdf.png"),
            )

            # Generate CCDF plot
            self.plot_ccdf(
                graph_execution_times,
                f"Graph '{graph_name}' Execution Times CCDF "
                f"({len(graph_execution_times)} tasks, {num_times_scheduled} sched)",
                "Execution Time (microseconds)",
                str(graphs_dir / f"{safe_graph_name}_execution_times_ccdf.png"),
            )

        # Generate overall graph comparison plots if there are multiple graphs
        if len(graph_tasks_map) > 1:
            self._generate_graph_comparison_plots(
                graph_tasks_map, graph_times_scheduled, graphs_dir
            )

    def _generate_graph_comparison_plots(
        self,
        graph_tasks_map: dict[str, list[dict[str, Any]]],
        graph_times_scheduled: dict[str, set],
        graphs_dir: Path,
    ) -> None:
        """Generate comparison plots showing all graphs together."""
        plt.figure(figsize=(12, 8))

        colors = [
            "blue",
            "red",
            "green",
            "orange",
            "purple",
            "brown",
            "pink",
            "gray",
            "olive",
            "cyan",
        ]
        color_idx = 0

        for graph_name in sorted(graph_tasks_map.keys()):
            tasks = graph_tasks_map[graph_name]
            execution_times = [task["duration_ns"] / 1000.0 for task in tasks]

            if (
                len(execution_times) < self.MIN_SAMPLES_FOR_PLOTS
            ):  # Skip graphs with too few samples
                continue

            # Calculate CDF
            values_sorted = np.sort(execution_times)
            y_vals = np.arange(1, len(values_sorted) + 1) / len(values_sorted)

            color = colors[color_idx % len(colors)]
            color_idx += 1

            num_scheduled = len(graph_times_scheduled[graph_name])
            plt.plot(
                values_sorted,
                y_vals,
                linewidth=2,
                color=color,
                label=f"{graph_name} (n={len(execution_times)}, {num_scheduled} sched)",
            )

        plt.grid(visible=True, alpha=0.3)
        plt.xlabel("Execution Time (microseconds)")
        plt.ylabel("P(X ≤ x)")
        plt.title("Graph Execution Times Comparison - CDF")
        plt.legend(loc="lower right")
        plt.xlim(left=0)
        plt.ylim(0, 1)

        plt.tight_layout()
        plt.savefig(
            str(graphs_dir / "all_graphs_comparison_cdf.png"),
            dpi=150,
            bbox_inches="tight",
        )
        self.plot_count += 1
        self.logger.info(
            f"[{self.plot_count}] Graph comparison CDF plot saved to "
            f"{graphs_dir / 'all_graphs_comparison_cdf.png'}"
        )
        plt.close()

        # Generate CCDF comparison
        plt.figure(figsize=(12, 8))
        color_idx = 0

        for graph_name in sorted(graph_tasks_map.keys()):
            tasks = graph_tasks_map[graph_name]
            execution_times = [task["duration_ns"] / 1000.0 for task in tasks]

            if (
                len(execution_times) < self.MIN_SAMPLES_FOR_PLOTS
            ):  # Skip graphs with too few samples
                continue

            # Calculate CCDF
            values_sorted = np.sort(execution_times)
            y_vals = 1 - (np.arange(1, len(values_sorted) + 1) / len(values_sorted))

            color = colors[color_idx % len(colors)]
            color_idx += 1

            num_scheduled = len(graph_times_scheduled[graph_name])
            plt.semilogy(
                values_sorted,
                y_vals,
                linewidth=2,
                color=color,
                label=f"{graph_name} (n={len(execution_times)}, {num_scheduled} sched)",
            )

        plt.grid(visible=True, alpha=0.3)
        plt.xlabel("Execution Time (microseconds)")
        plt.ylabel("P(X > x) [log scale]")
        plt.title("Graph Execution Times Comparison - CCDF")
        plt.legend(loc="upper right")
        plt.xlim(left=0)

        plt.tight_layout()
        plt.savefig(
            str(graphs_dir / "all_graphs_comparison_ccdf.png"),
            dpi=150,
            bbox_inches="tight",
        )
        self.plot_count += 1
        self.logger.info(
            f"[{self.plot_count}] Graph comparison CCDF plot saved to "
            f"{graphs_dir / 'all_graphs_comparison_ccdf.png'}"
        )
        plt.close()

    def _generate_trigger_plots(self, output_dir: Path) -> None:
        """Generate trigger-specific plots."""
        output_dir.mkdir(parents=True, exist_ok=True)

        # Get trigger data
        execution_times = self.get_trigger_execution_times_us()
        jitters = self.get_trigger_jitters_us()

        # Generate overall trigger plots
        self.plot_cdf(
            execution_times,
            f"Overall Trigger Callback Duration CDF ({len(execution_times)} samples)",
            "Callback Duration (microseconds)",
            str(output_dir / "overall_execution_times_cdf.png"),
        )

        self.plot_ccdf(
            execution_times,
            f"Overall Trigger Callback Duration CCDF ({len(execution_times)} samples)",
            "Callback Duration (microseconds)",
            str(output_dir / "overall_execution_times_ccdf.png"),
        )

        self.plot_cdf(
            jitters,
            f"Overall Trigger Latency CDF ({len(jitters)} samples)",
            "Trigger Latency (microseconds, absolute)",
            str(output_dir / "overall_jitter_cdf.png"),
        )

        self.plot_ccdf(
            jitters,
            f"Overall Trigger Latency CCDF ({len(jitters)} samples)",
            "Trigger Latency (microseconds, absolute)",
            str(output_dir / "overall_jitter_ccdf.png"),
        )

        # Generate plots for different time ranges
        total_triggers = len(self.trigger_records)
        if total_triggers < self.MIN_TRIGGERS_FOR_SEGMENTS:
            self.logger.info("Too few trigger records for meaningful time segment plots")
            return

        # Create subdirectory for time segments
        segments_dir = output_dir / "time_segments"
        segments_dir.mkdir(parents=True, exist_ok=True)

        # Generate plots for different segments of trigger execution
        segment_size = max(1000, total_triggers // 10)  # At least 1000 or 10% of records

        for i in range(0, total_triggers, segment_size):
            end_idx = min(i + segment_size, total_triggers)
            segment_records = self.trigger_records[i:end_idx]

            if len(segment_records) < self.MIN_SAMPLES_FOR_PLOTS:
                continue

            segment_name = f"triggers_{i + 1}-{end_idx}"

            # Callback durations for this segment
            callback_durations = [r["callback_duration_ns"] / 1000.0 for r in segment_records]
            latencies = [abs(r["latency_ns"]) / 1000.0 for r in segment_records]

            # Generate CDF plots
            self.plot_cdf(
                callback_durations,
                f"Triggers {i + 1}-{end_idx} Callback Duration CDF "
                f"({len(callback_durations)} samples)",
                "Callback Duration (microseconds)",
                str(segments_dir / f"{segment_name}_callback_duration_cdf.png"),
            )

            self.plot_cdf(
                latencies,
                f"Triggers {i + 1}-{end_idx} Latency CDF ({len(latencies)} samples)",
                "Latency (microseconds, absolute)",
                str(segments_dir / f"{segment_name}_latency_cdf.png"),
            )

            # Generate CCDF plots
            self.plot_ccdf(
                callback_durations,
                f"Triggers {i + 1}-{end_idx} Callback Duration CCDF "
                f"({len(callback_durations)} samples)",
                "Callback Duration (microseconds)",
                str(segments_dir / f"{segment_name}_callback_duration_ccdf.png"),
            )

            self.plot_ccdf(
                latencies,
                f"Triggers {i + 1}-{end_idx} Latency CCDF ({len(latencies)} samples)",
                "Latency (microseconds, absolute)",
                str(segments_dir / f"{segment_name}_latency_ccdf.png"),
            )

    def _group_tasks_by_graph(
        self,
    ) -> tuple[dict[str, list[dict[str, Any]]], dict[str, set]]:
        """Group task records by graph name and track unique times scheduled."""
        graph_tasks_map = {}
        graph_times_scheduled = {}

        for record in self.task_records:
            graph_name = record["graph_name"]
            times_scheduled = record["times_scheduled"]

            # Group tasks by graph
            if graph_name not in graph_tasks_map:
                graph_tasks_map[graph_name] = []
            graph_tasks_map[graph_name].append(record)

            # Track unique times scheduled per graph
            if graph_name not in graph_times_scheduled:
                graph_times_scheduled[graph_name] = set()
            graph_times_scheduled[graph_name].add(times_scheduled)

        return graph_tasks_map, graph_times_scheduled

    def _print_trigger_summary(self) -> None:
        """Print trigger execution statistics summary."""
        self.logger.info("====== TRIGGER EXECUTION RECORDS ======")
        self.logger.info("====== Trigger Execution Statistics ======")
        self.logger.info(f"Total triggers executed: {len(self.trigger_records)}")

        # Calculate basic statistics
        callback_durations = [r["callback_duration_ns"] / 1000.0 for r in self.trigger_records]
        latencies = [abs(r["latency_ns"]) / 1000.0 for r in self.trigger_records]

        # Callback duration statistics
        callback_stats = self.calculate_statistics(callback_durations)
        self._log_statistics(callback_stats, "Callback Duration Distribution")

        # Latency statistics
        latency_stats = self.calculate_statistics(latencies)
        self._log_statistics(latency_stats, "Trigger Latency Distribution")

        # Additional trigger-specific metrics if available
        if self.trigger_records:
            first_trigger = self.trigger_records[0]["trigger_count"]
            last_trigger = self.trigger_records[-1]["trigger_count"]
            total_span = last_trigger - first_trigger + 1
            missed_triggers = total_span - len(self.trigger_records)

            self.logger.info(
                f"Trigger Range: {first_trigger} to {last_trigger} (span: {total_span})"
            )
            if missed_triggers > 0:
                self.logger.info(
                    f"Missed/Skipped triggers: {missed_triggers} "
                    f"({missed_triggers / total_span * 100:.1f}%)"
                )

        self.logger.info("======================================")


def main() -> None:
    """Analyze task and trigger execution statistics from JSON files."""
    logger = logging.getLogger(__name__)

    # Track if any warnings or errors occurred
    warning_error_handler = WarningErrorHandler()

    parser = argparse.ArgumentParser(
        description="Analyze task and trigger execution statistics from JSON files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Standalone with uv (no installation required):
    uv run task_stats.py task_stats.json
    uv run task_stats.py mixed_stats.json --output-dir ./analysis

    # Direct Python execution:
    python task_stats.py task_stats.json
    python task_stats.py trigger_stats.json --no-plots

    # After pip install:
    task_stats task_stats.json --verbose
    task_stats mixed_stats.json --output-dir ./results
        """,
    )

    parser.add_argument(
        "json_file",
        type=Path,
        help="Path to JSON statistics file generated by TaskMonitor or TimedTrigger",
    )
    parser.add_argument(
        "--output-dir",
        "-o",
        type=Path,
        help="Output directory for plot files (default: <input_filename>_analysis/)",
    )
    parser.add_argument(
        "--no-plots",
        action="store_true",
        help="Skip generating plots, only print summary",
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Configure logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s.%(msecs)03d %(levelname)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    # Add the warning/error handler to the root logger
    root_logger = logging.getLogger()
    root_logger.addHandler(warning_error_handler)

    if not args.json_file.exists():
        logger.error(f"File {args.json_file} does not exist")
        sys.exit(1)

    # Create analyzer and load data
    analyzer = TaskStatsAnalyzer(args.json_file)
    if not analyzer.load_data():
        sys.exit(1)

    # Print summary statistics
    analyzer.print_summary()

    # Generate plots if requested
    if not args.no_plots:
        output_dir = args.output_dir or (args.json_file.parent / f"{args.json_file.stem}_analysis")

        # Create output directory
        output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Generating plots in directory: {output_dir}")

        # Generate overall plots
        analyzer.generate_plots(output_dir)

        logger.info(f"Analysis complete! Generated plots in directory: {output_dir}")
    else:
        logger.info("Analysis complete!")

    # Exit with error code if any warnings or errors occurred
    if warning_error_handler.has_warning_or_error():
        logger.error("Analysis completed with warnings or errors - exiting with error code 1")
        sys.exit(1)


if __name__ == "__main__":
    main()
